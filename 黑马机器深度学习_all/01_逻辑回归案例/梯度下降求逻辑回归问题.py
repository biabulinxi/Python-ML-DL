# -*- coding: utf-8 -*-
# @Project:AID1810
# @Author:biabu
# @Date:2019/3/19 19:29
# @File_name:æ¢¯åº¦ä¸‹é™æ±‚é€»è¾‘å›å½’é—®é¢˜.py
# @IDE:PyCharm

"""
æ„å»ºä¸€ä¸ªæ¢¯åº¦ä¸‹é™æ³•æ±‚è§£é€»è¾‘å›å½’é—®é¢˜çš„ç®—æ³•
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
import time


class LogiReg(object):
    """
    ç›®æ ‡ï¼šå»ºç«‹åˆ†ç±»å™¨ï¼ˆæ±‚è§£å‡ºä¸‰ä¸ªå‚æ•° theta0, theta1, theta2 ï¼‰
    è®¾å®šé˜ˆå€¼ï¼Œæ ¹æ®é˜ˆå€¼åˆ¤æ–­å½•å–ç»“æœã€‚

    è¦å®Œæˆçš„æ¨¡å—
    sigmoid : æ˜ å°„åˆ°æ¦‚ç‡çš„å‡½æ•°
    model : è¿”å›é¢„æµ‹ç»“æœå€¼
    cost : æ ¹æ®å‚æ•°è®¡ç®—æŸå¤±
    gradient : è®¡ç®—æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦æ–¹å‘
    descent : è¿›è¡Œå‚æ•°æ›´æ–°
    accuracy: è®¡ç®—ç²¾åº¦
    """

    def __init__(self, data):
        self.data = data
        self.STOP_ITER = 0
        self.STOP_COST = 1
        self.STOP_GRAD = 2

    def _sctter(self):
        """
        :return: æµ‹è¯•æ•°æ®ï¼Œç»˜åˆ¶æ•£ç‚¹å›¾
        """
        positive = self.data[self.data[
                                 'Admitted'] == 1]  # returns the subset of rows such Admitted = 1, i.e. the set of *positive* examples
        negative = self.data[self.data[
                                 'Admitted'] == 0]  # returns the subset of rows such Admitted = 0, i.e. the set of *negative* examples

        fig, ax = plt.subplots(figsize=(10, 5))
        ax.scatter(positive['Exam1'], positive['Exam2'], s=30, c='b', marker='o', label='Admitted')
        ax.scatter(negative['Exam1'], negative['Exam2'], s=30, c='b', marker='o', label='Not Admitted')
        ax.legend()
        ax.set_xlabel('Exam1 Score')
        ax.set_ylabel('Exam2 Score')

    def sigmoid(self, z):
        """
        Sigmoid
        g:â„â†’[0,1]
        g(0)=0.5
        g(âˆ’âˆ)=0
        g(+âˆ)=1
        :param z: è‡ªç¼–é‡
        :return: sigmoid å€¼
        """
        return 1 / (1 + np.exp(-z))

    def model(self, X, theta):
        """
        :param X: è‡ªå˜é‡æ•°æ®çŸ©é˜µ
        :param theta: å¾…æ±‚çš„ç³»æ•°çŸ©é˜µ
        :return: è¿”å›é¢„æµ‹ç»“æœå€¼
        """
        return self.sigmoid(np.dot(X, theta.T))

    def cost(self, X, y, theta):
        """
        å°†å¯¹æ•°ä¼¼ç„¶å‡½æ•°å»è´Ÿå·
        D(â„(ğ‘¥),ğ‘¦)=âˆ’ğ‘¦log(â„(ğ‘¥))âˆ’(1âˆ’ğ‘¦)log(1âˆ’â„(ğ‘¥))
        æ±‚å¹³å‡æŸå¤±
        J(theta)=âˆ‘D(â„(ğ‘¥),ğ‘¦)
        :param X: è‡ªå˜é‡çŸ©é˜µ
        :param y: å› å˜é‡çŸ©é˜µ
        :param theta:  å¾…æ±‚çš„å‚æ•°çŸ©é˜µ
        :return: å¯¹æ•°ä¼¼ç„¶çš„å¹³å‡æŸå¤±
        """
        left = np.multiply(-y, np.log(self.model(X, theta)))  # è¡¨ç¤ºå‡å·å·¦è¾¹çš„å¼å­
        right = np.multiply(1 - y, np.log(1 - self.model(X, theta)))
        return np.sum(left - right) / len(X)

    def gradient(self, X, y, theta):
        """
        å…¬å¼:
        âˆ‚J/âˆ‚theta_j=âˆ’1/m * âˆ‘i=1_n(ğ‘¦_iâˆ’â„_theta(ğ‘¥_i))ğ‘¥_i_j
        :param X: è‡ªå˜é‡çŸ©é˜µ
        :param y: å› å˜é‡çŸ©é˜µ
        :param theta:  å¾…æ±‚çš„å‚æ•°çŸ©é˜µ
        :return: è®¡ç®—æŸå¤±å‡½æ•°çš„æ¢¯åº¦å€¼
        """
        grad = np.zeros(theta.shape)
        error = (self.model(X, theta) - y).ravel()
        for j in range(len(theta.ravel())):  # éå†æ¯ä¸€ä¸ªç³»æ•°å‚æ•°
            term = np.multiply(error, X[:, j])
            grad[0, j] = np.sum(term) / len(X)
        return grad

    def descent(self, data, theta, batchSize, stopType, thresh, alpha):
        """
        æ¢¯åº¦ä¸‹é™çš„æ±‚è§£ï¼Œè¿›è¡Œå‚æ•°çš„æ›´æ–°
        :param data: æ•°æ®
        :param batchSize:  æ‰¹å¤„ç†å¤§å°
        :param stopType:  åœæ­¢è¿­ä»£çš„ç±»å‹
        :param thresh:  åœæ­¢è¿­ä»£çš„é˜ˆå€¼
        :param alpha:  æ¢¯åº¦ä¸‹é™çš„æ­¥é•¿å¤§å°
        :return:
        """

        def stopCriterion(type, value, threshold):
            """
            è®¾ç½®ä¸‰ç§ä¸åŒçš„åœæ­¢è¿­ä»£çš„ç­–ç•¥
            :param type: åœæ­¢è¿­ä»£çš„ç±»å‹
            :param value:  è¿­ä»£å€¼
            :param threshold:  é˜ˆå€¼
            :return:
            """
            if type == self.STOP_ITER:
                return value > threshold
            elif type == self.STOP_COST:
                return abs(value[-1] - value[-2]) < threshold
            elif type == self.STOP_GRAD:
                return np.linalg.norm(value) < threshold

        def shuffleData(data):
            """
            å°†æ•°æ®æ‰“ä¹±ï¼Œè¿›è¡Œæ´—ç‰Œ
            :param data:
            :return:
            """
            np.random.shuffle(data)
            cols = data.shape[1]
            X = data[:, 0:cols - 1]
            y = data[:, cols - 1:]
            return X, y

        init_time = time.time()
        i = 0  # è¿­ä»£æ¬¡æ•°
        k = 0  # batch
        X, y = shuffleData(data)
        grad = np.zeros(theta.shape)  # åˆå§‹åŒ–æ¢¯åº¦çŸ©é˜µ
        costs = [self.cost(X, y, theta)]  # æŸå¤±å€¼

        # è¿›è¡Œè¿­ä»£
        while True:
            grad = self.gradient(X[k:k + batchSize], y[k:k + batchSize], theta)
            k += batchSize  # å–batchæ•°é‡ä¸ªæ•°æ®
            if k >= batchSize:
                k = 0
                X, y = shuffleData(data)  # é‡æ–°æ´—ç‰Œ
            theta = theta - alpha * grad  # å‚æ•°æ›´æ–°
            costs.append(self.cost(X, y, theta))  # è®¡ç®—æ–°çš„æŸå¤±
            i += 1

            if stopType == self.STOP_ITER:
                value = i
            elif stopType == self.STOP_COST:
                value = costs
            elif stopType == self.STOP_GRAD:
                value = grad

            if stopCriterion(stopType, value, thresh):
                break
        return theta, i - 1, costs, grad, time.time() - init_time

    def runExpe(self, data, theta, batchSize, stopType, thresh, alpha):
        """
        è°ƒç”¨è¿­ä»£å‡½æ•°ï¼Œå¹¶è¿›è¡Œç»˜å›¾
        :param data:
        :param theta:
        :param batchSize:
        :param stopType:
        :param thresh:
        :param alpha:
        :return:
        """
        theta, iter, costs, grad, dur = self.descent(data, theta, batchSize, stopType, thresh, alpha)
        name = 'Original' if (data[:, 1] > 2).sum() > 1 else 'Scaled'
        name += 'data - learning rate: {} -'.format(alpha)
        if batchSize == batchSize:
            strDescType = 'Gradient'
        elif batchSize == 1:
            strDescType = 'Stochatic'
        else:
            strDescType = 'Mini-bath({})'.format(batchSize)
        name += strDescType + ' descent - Stop:'
        if stopType == self.STOP_ITER:
            strStop = "{} iterations".format(thresh)
        elif stopType == self.STOP_COST:
            strStop = "costs change < {}".format(thresh)
        else:
            strStop = "gradient norm < {}".format(thresh)
        name += strStop
        print("***{}\nTheta: {} - Iter: {} - Last cost: {:03.2f} - Duration: {:03.2f}s".format(name, theta, iter,
                                                                                               costs[-1], dur))
        fig, ax = plt.subplots(figsize=(12, 4))
        ax.plot(np.arange(len(costs)), costs, 'r')
        ax.set_xlabel('Iterations')
        ax.set_ylabel('Cost')
        ax.set_title(name.upper() + ' - Error vs. Iteration')
        plt.show()
        return theta

    def predict(self, X, theta):
        """
        è®¡ç®—æ¨¡å‹çš„ç²¾åº¦
        """
        return [1 if x >= 0.5 else 0 for x in self.model(X, theta)]


if __name__ == '__main__':
    path = 'data' + os.sep + 'LogiReg_data.txt'
    data = pd.read_csv(path, header=None, names=['Exam1', 'Exam2', 'Admitted'])
    data.head()
    data.insert(0, 'Ones', 1)
    orig_data = data.as_matrix()
    theta = np.zeros([1, 3])

    # æ•°æ®æ ‡å‡†åŒ–
    from sklearn import preprocessing as pp

    scaled_data = orig_data.copy()
    scaled_data[:, 1:3] = pp.scale(orig_data[:, 1:3])

    # åˆ›å»ºå¯¹è±¡ï¼Œè¿›è¡Œè®¡ç®—
    logireg = LogiReg(data)
    logireg.runExpe(orig_data, theta, batchSize=100, stopType=0, thresh=5000, alpha=0.000001)

    # æ±‚è§£æ¨¡å‹ç²¾åº¦
    scaled_X = scaled_data[:, :3]
    y = scaled_data[:, 3]
    predictions = logireg.predict(scaled_X, theta)
    correct = [1 if ((a == 1 and b == 1) or (a == 0 and b == 0)) else 0 for (a, b) in zip(predictions, y)]
    accuracy = (sum(map(int, correct)) % len(correct))
    print('accuracy = {0}%'.format(accuracy))
